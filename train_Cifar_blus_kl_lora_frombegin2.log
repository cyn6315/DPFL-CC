nohup: ignoring input
cuda:3
Namespace(alpha=1, batch_size=100, bn_sparsity=0.9, clip_bound=1.8, clipping_style='all-layer', cluster_temperature=1.0, dataset='CIFAR-10', dataset_dir='/home/chenyannan/fast-differential-privacy-main/examples/image_classification/data', downsample_lr=0.05, epochs=55, epsilon=8, feature_dim=128, global_lr=1.06, image_size=224, instance_temperature=0.5, kl_threshold=0.4, learning_rate=0.004, linear_sparsity=0.75, local_epoch=6, loss_KL=0.5, mini_bs=100, miu=0.05, model_path='save/Cifar-10-DPFL-ResNet18-blur-lus-kl-threshold-lorabegin2', momentum=0.3, n_clients=600, project_lr=0.05, r_conv=6, r_proj=16, relation_t=1, relation_t2=1, reload=False, resnet='ResNet18_lora', resnet_lr=0.14, sample_ratio=0.5, seed=17, smooth_K=6, smooth_loss_radius=2, smooth_step=0, start_epoch=0, test_image_size=256, weight_decay=1e-05, workers=8)
save/Img-10-pretrain-transform/checkpoint_532.tar
resnet.conv1.weight 9408 torch.Size([64, 3, 7, 7])
resnet.conv1.lora_A 882 torch.Size([42, 21])
resnet.conv1.lora_B 18816 torch.Size([448, 42])
resnet.bn1.weight 64 torch.Size([64])
resnet.bn1.bias 64 torch.Size([64])
resnet.layer1.0.conv1.weight 36864 torch.Size([64, 64, 3, 3])
resnet.layer1.0.conv1.lora_A 3456 torch.Size([18, 192])
resnet.layer1.0.conv1.lora_B 3456 torch.Size([192, 18])
resnet.layer1.0.bn1.weight 64 torch.Size([64])
resnet.layer1.0.bn1.bias 64 torch.Size([64])
resnet.layer1.0.conv2.weight 36864 torch.Size([64, 64, 3, 3])
resnet.layer1.0.conv2.lora_A 3456 torch.Size([18, 192])
resnet.layer1.0.conv2.lora_B 3456 torch.Size([192, 18])
resnet.layer1.0.bn2.weight 64 torch.Size([64])
resnet.layer1.0.bn2.bias 64 torch.Size([64])
resnet.layer1.1.conv1.weight 36864 torch.Size([64, 64, 3, 3])
resnet.layer1.1.bn1.weight 64 torch.Size([64])
resnet.layer1.1.bn1.bias 64 torch.Size([64])
resnet.layer1.1.conv2.weight 36864 torch.Size([64, 64, 3, 3])
resnet.layer1.1.bn2.weight 64 torch.Size([64])
resnet.layer1.1.bn2.bias 64 torch.Size([64])
resnet.layer2.0.conv1.weight 73728 torch.Size([128, 64, 3, 3])
resnet.layer2.0.conv1.lora_A 3456 torch.Size([18, 192])
resnet.layer2.0.conv1.lora_B 6912 torch.Size([384, 18])
resnet.layer2.0.bn1.weight 128 torch.Size([128])
resnet.layer2.0.bn1.bias 128 torch.Size([128])
resnet.layer2.0.conv2.weight 147456 torch.Size([128, 128, 3, 3])
resnet.layer2.0.conv2.lora_A 6912 torch.Size([18, 384])
resnet.layer2.0.conv2.lora_B 6912 torch.Size([384, 18])
resnet.layer2.0.bn2.weight 128 torch.Size([128])
resnet.layer2.0.bn2.bias 128 torch.Size([128])
resnet.layer2.0.downsample.0.weight 8192 torch.Size([128, 64, 1, 1])
resnet.layer2.0.downsample.0.lora_A 2304 torch.Size([36, 64])
resnet.layer2.0.downsample.0.lora_B 4608 torch.Size([128, 36])
resnet.layer2.0.downsample.1.weight 128 torch.Size([128])
resnet.layer2.0.downsample.1.bias 128 torch.Size([128])
resnet.layer2.1.conv1.weight 147456 torch.Size([128, 128, 3, 3])
resnet.layer2.1.bn1.weight 128 torch.Size([128])
resnet.layer2.1.bn1.bias 128 torch.Size([128])
resnet.layer2.1.conv2.weight 147456 torch.Size([128, 128, 3, 3])
resnet.layer2.1.bn2.weight 128 torch.Size([128])
resnet.layer2.1.bn2.bias 128 torch.Size([128])
resnet.layer3.0.conv1.weight 294912 torch.Size([256, 128, 3, 3])
resnet.layer3.0.conv1.lora_A 6912 torch.Size([18, 384])
resnet.layer3.0.conv1.lora_B 13824 torch.Size([768, 18])
resnet.layer3.0.bn1.weight 256 torch.Size([256])
resnet.layer3.0.bn1.bias 256 torch.Size([256])
resnet.layer3.0.conv2.weight 589824 torch.Size([256, 256, 3, 3])
resnet.layer3.0.conv2.lora_A 13824 torch.Size([18, 768])
resnet.layer3.0.conv2.lora_B 13824 torch.Size([768, 18])
resnet.layer3.0.bn2.weight 256 torch.Size([256])
resnet.layer3.0.bn2.bias 256 torch.Size([256])
resnet.layer3.0.downsample.0.weight 32768 torch.Size([256, 128, 1, 1])
resnet.layer3.0.downsample.0.lora_A 4608 torch.Size([36, 128])
resnet.layer3.0.downsample.0.lora_B 9216 torch.Size([256, 36])
resnet.layer3.0.downsample.1.weight 256 torch.Size([256])
resnet.layer3.0.downsample.1.bias 256 torch.Size([256])
resnet.layer3.1.conv1.weight 589824 torch.Size([256, 256, 3, 3])
resnet.layer3.1.conv1.lora_A 13824 torch.Size([18, 768])
resnet.layer3.1.conv1.lora_B 13824 torch.Size([768, 18])
resnet.layer3.1.bn1.weight 256 torch.Size([256])
resnet.layer3.1.bn1.bias 256 torch.Size([256])
resnet.layer3.1.conv2.weight 589824 torch.Size([256, 256, 3, 3])
resnet.layer3.1.conv2.lora_A 13824 torch.Size([18, 768])
resnet.layer3.1.conv2.lora_B 13824 torch.Size([768, 18])
resnet.layer3.1.bn2.weight 256 torch.Size([256])
resnet.layer3.1.bn2.bias 256 torch.Size([256])
resnet.layer4.0.conv1.weight 1179648 torch.Size([512, 256, 3, 3])
resnet.layer4.0.conv1.lora_A 13824 torch.Size([18, 768])
resnet.layer4.0.conv1.lora_B 27648 torch.Size([1536, 18])
resnet.layer4.0.bn1.weight 512 torch.Size([512])
resnet.layer4.0.bn1.bias 512 torch.Size([512])
resnet.layer4.0.conv2.weight 2359296 torch.Size([512, 512, 3, 3])
resnet.layer4.0.conv2.lora_A 27648 torch.Size([18, 1536])
resnet.layer4.0.conv2.lora_B 27648 torch.Size([1536, 18])
resnet.layer4.0.bn2.weight 512 torch.Size([512])
resnet.layer4.0.bn2.bias 512 torch.Size([512])
resnet.layer4.0.downsample.0.weight 131072 torch.Size([512, 256, 1, 1])
resnet.layer4.0.downsample.0.lora_A 9216 torch.Size([36, 256])
resnet.layer4.0.downsample.0.lora_B 18432 torch.Size([512, 36])
resnet.layer4.0.downsample.1.weight 512 torch.Size([512])
resnet.layer4.0.downsample.1.bias 512 torch.Size([512])
resnet.layer4.1.conv1.weight 2359296 torch.Size([512, 512, 3, 3])
resnet.layer4.1.conv1.lora_A 27648 torch.Size([18, 1536])
resnet.layer4.1.conv1.lora_B 27648 torch.Size([1536, 18])
resnet.layer4.1.bn1.weight 512 torch.Size([512])
resnet.layer4.1.bn1.bias 512 torch.Size([512])
resnet.layer4.1.conv2.weight 2359296 torch.Size([512, 512, 3, 3])
resnet.layer4.1.conv2.lora_A 27648 torch.Size([18, 1536])
resnet.layer4.1.conv2.lora_B 27648 torch.Size([1536, 18])
resnet.layer4.1.bn2.weight 512 torch.Size([512])
resnet.layer4.1.bn2.bias 512 torch.Size([512])
instance_projector.0.weight 262144 torch.Size([512, 512])
instance_projector.0.bias 512 torch.Size([512])
instance_projector.2.weight 65536 torch.Size([128, 512])
instance_projector.2.bias 128 torch.Size([128])
cluster_projector.0.weight 262144 torch.Size([512, 512])
cluster_projector.0.bias 512 torch.Size([512])
cluster_projector.2.weight 5120 torch.Size([10, 512])
cluster_projector.2.bias 10 torch.Size([10])
sigma: 3.857421875
Number of total parameters:  12189756
Number of trainable p  arameters:  1022844
### Creating features from model ###
Step [0/120]	 Computing features...
Step [20/120]	 Computing features...
Step [40/120]	 Computing features...
Step [60/120]	 Computing features...
Step [80/120]	 Computing features...
Step [100/120]	 Computing features...
Global  NMI = 0.2686 ARI = 0.1508 F = 0.2619 ACC = 0.4015
Average NMI = 0.3889 ARI = 0.1523 F = 0.2633 ACC = 0.4258
Norm:  tensor(3.4772, device='cuda:3')
Round:  0 User: 0 Train Loss: 7.557
Norm:  tensor(3.5153, device='cuda:3')
Round:  0 User: 1 Train Loss: 7.461
Norm:  tensor(3.4734, device='cuda:3')
Round:  0 User: 2 Train Loss: 7.459
Norm:  tensor(3.6945, device='cuda:3')
Round:  0 User: 3 Train Loss: 7.694
Norm:  tensor(3.4359, device='cuda:3')
Round:  0 User: 4 Train Loss: 7.462
Norm:  tensor(3.6316, device='cuda:3')
Round:  0 User: 5 Train Loss: 7.704
Norm:  tensor(3.8863, device='cuda:3')
Round:  0 User: 6 Train Loss: 7.748
Norm:  tensor(4.1689, device='cuda:3')
Round:  0 User: 7 Train Loss: 8.059
Norm:  tensor(3.0751, device='cuda:3')
Round:  0 User: 8 Train Loss: 7.265
Norm:  tensor(4.0414, device='cuda:3')
Round:  0 User: 9 Train Loss: 7.846
Norm:  tensor(3.8791, device='cuda:3')
Round:  0 User: 10 Train Loss: 7.824
Norm:  tensor(4.0039, device='cuda:3')
Round:  0 User: 11 Train Loss: 7.970
Norm:  tensor(4.6495, device='cuda:3')
Round:  0 User: 12 Train Loss: 8.152
